\chapter{Multivariate analysis}
\label{ch:nn_mva}
The reconstruction of the top quark in the s-channel is done by multivariate analysis, which combines several input variables to a final discriminator, yielding better results than the once popular cut-and-count analysis. This chapter gives a theoretical introduction to artificial neural networks and boosted decision trees as multivariate analysis tools that will be used in this thesis.

\section{Artificial Neural Networks}
Artificial Neural Networks (ANN) represent a class of algorithms widely used in machine learning and pattern recognition, modeled after the structure of the biological brain. The power of ANNs lies in being able to identify complex relationships between given inputs and outputs, recognizing them in other input values and generating an accurate prediction.

ANNs consist of a sequence of layers containing neurons. The first layer is the \emph{input layer} with $n$ neurons, accepting input values $\va{x}=(x_1,x_2,...,x_n)$. These neurons forward the input values to the neurons of the next layer. The last layer is the \emph{output layer}, which returns the predicted values of the ANN. Between these layers, one or more \emph{hidden layers} may exist. ANNs with two hidden layers or more are known as deep neural networks (DNN). This class of networks with more than one layer of computational units is generally known generally as a multi-layer perceptron. A single-layer perceptron has only a single layer of output nodes, the inputs are directly fed to the outputs via a series of weights.

The type of neural network used in the thesis is a feed-forward neural network, where the output values are passed on from one layer to the next and do not return to any neuron of the previous layer (Figure \ref{fig:neural_network}).
\begin{figure}[H]
    \centering
    \input{assets/chap03/neuralnetwork.tex}
    \caption{Example of a fully-connected neural network with one input layer containing 4 input neurons, two hidden layers containing 5 neurons each and one output layer containing a single neuron. The output of a neuron serves as input for all the neurons of the next layer.}
    \label{fig:neural_network}
\end{figure}
A neuron is a single computational unit, that receives the input values $\va{x}={x_1,x_2,...,x_n}$ from neurons in the previous layer and outputs a single value $y$ by the rule
\begin{equation}
    y=f\left( \sum_{k=1}^{n} w_k \cdot x_k + b_k \right).
\end{equation}
Each input value $x_k$ is multiplied with adaptable weights $w_k$ and an adaptable bias $b_k$ is added. An activation function $f$ is applied to the result, which introduces non-linear properties to the neuron. Without the activation function, the neural network would simply be a linear regression model and would not be able to learn and model complex data sets. Commonly used activation functions are the ReLu (rectified linear unit), Elu (exponential linear unit) and sigmoid function.

\subsection{Training}
To calculate the correct neuron weights and biases, an ANN needs to be trained with a set of predefined input and output values. The available training data is split into \emph{training}, \emph{validation} and \emph{test} data. The training and validation data are both used during training, with validation data being used for determining overfitting, e.g., an accuracy increase over the training data, but an unchanged accuracy over validation data being such an indication. The testing data set is applied to the neural network after training to confirm its predictive power.

A loss or cost function $C$ that measures the inconsistency between predicted value $\hat{y}$ and actual label $y$ is used to adapt the weight $w$, such that the value of the loss function is minimized (or reaches 0). 

Examples of loss functions are the mean squared error (MSE)
\begin{equation}\label{eq:mse}
C_{MSE}=\frac{1}{N} \sum_{i=1}^N (y_i-\hat{y}_i)^2,
\end{equation}
and the binary cross entropy
\begin{equation}\label{eq:cross_entropy}
C_{E}=-\sum_{i=1}^N \left(y_i-\ln{(\hat{y}_i)+(1-y_i)\ln{(1-\hat{y}_i)}}\right),
\end{equation}
where $N$ denotes the number of the training data sets in both equations.

The optimization of the loss function is done using algorithms, such as gradient descent, stochastic gradient descent, Adam optimizer and others.

The step size, by which the weights are adjusted, is called the \emph{learning rate} $\eta$, and must be fine-tuned before the training
\begin{equation}
    w \leftarrow w - \eta \dv{C}{w}.
\end{equation}
Higher learning rates make it difficult to determine the minimum of the cost function accurately, whereas lower learning rates may impact the performance of the algorithm.

The most common algorithm for weight adjustment is the backpropagation algorithm \cite{Rumelhart:1988:LRB:65669.104451}. In very broad terms, backpropagation iteratively adjusts each weight in the network proportionally to its overall error contribution. Starting at the output layer and going backwards, the chain rule of differentiation is applied for finding the derivatives of cost.

\subsection{Regularization}
\section{Boosted Decision Trees}

\begin{center}
    \begin{tabular}{ll}
    \hline
    Variable & Description\\
    \hline
    $p_T(t)$ & Transverse momentum of top signal\\
    $\eta_T(t)$ & Pseudorapidity of top signal\\
    $\phi_T(t)$ & $\phi$ of top signal\\
    $m_T(t)$ & Invariant mass of top signal\\

    $p_T(j)$ & Transverse momentum of jet signal\\
    $\eta_T(j)$ & Pseudorapidity of jet signal\\
    $\phi_T(j)$ & $\phi$ of jet signal\\
    $m_T(j)$ & Invariant mass of jet signal\\

    $p_T(W)$ & Transverse momentum of W boson\\
    $\eta_T(W)$ & Pseudorapidity of W boson\\
    $\phi_T(W)$ & $\phi$ of W boson\\
    $m_T(W)$ & Invariant mass of W boson\\

    number of jets & Number of additional jets with more than \SI{20}{GeV}\\
  \end{tabular}
\end{center}
